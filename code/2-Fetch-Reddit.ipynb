{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit get-comment tool, covid-19 sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "seed = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "comment_url = 'https://api.pushshift.io/reddit/search/comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with different localities here\n",
    "subreddits = ['nyc', 'houston']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields to fetch from Reddit API\n",
    "submission_fields = ['id','title', 'created_utc','num_comments','subreddit']\n",
    "comment_fields = ['link_id','body','created_utc', 'subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    start_time = round(time.time())\n",
    "    res = requests.get(\n",
    "        sub_url,\n",
    "        params={\n",
    "            'subreddit' : subreddit,\n",
    "            'q' : 'covid|quarantine|pandemic|coronavirus',\n",
    "            'fields': submission_fields,\n",
    "            'size' : 400,\n",
    "            'sort_type' : 'num_comments',\n",
    "            'sort' : 'desc',\n",
    "            'before': start_time,  # We can also manually set this \n",
    "            'after': '80d',      # With this on, the unbalanced classes get moreso\n",
    "        })\n",
    "    # Make sure we got a 2xx response\n",
    "    res.raise_for_status()\n",
    "\n",
    "    df = pd.DataFrame(res.json()['data'])\n",
    "    \n",
    "    # Filter out non-commented; could also set 'sort_type' parameter to get most commented\n",
    "    df = df[df['num_comments'] > 0]\n",
    "    \n",
    "    df_list.append(df)\n",
    "\n",
    "# Put all posts in one data frame\n",
    "submissions = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Convert date to YYYY-MM-DD format\n",
    "submissions['date'] = [dt.date.fromtimestamp(x).isoformat() for x in submissions['created_utc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get lists of link ids from each subreddit\n",
    "link_ids = {sub: submissions[submissions[\"subreddit\"] == sub][\"id\"] for sub in subreddits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 comments from r/nyc\n",
      "Fetched 2000 comments from r/nyc\n",
      "Fetched 3000 comments from r/nyc\n",
      "Fetched 4000 comments from r/nyc\n",
      "Fetched 5000 comments from r/nyc\n",
      "Fetched 6000 comments from r/nyc\n",
      "Fetched 7000 comments from r/nyc\n",
      "Fetched 8000 comments from r/nyc\n",
      "Fetched 9000 comments from r/nyc\n",
      "Fetched 10000 comments from r/nyc\n",
      "Fetched 11000 comments from r/nyc\n",
      "Fetched 12000 comments from r/nyc\n",
      "Fetched 13000 comments from r/nyc\n",
      "Fetched 14000 comments from r/nyc\n",
      "Fetched 15000 comments from r/nyc\n",
      "Fetched 16000 comments from r/nyc\n",
      "Fetched 17000 comments from r/nyc\n",
      "Fetched 18000 comments from r/nyc\n",
      "Fetched 19000 comments from r/nyc\n",
      "Fetched 20000 comments from r/nyc\n",
      "Fetched 21000 comments from r/nyc\n",
      "Fetched 22000 comments from r/nyc\n",
      "Fetched 23000 comments from r/nyc\n",
      "Fetched 24000 comments from r/nyc\n",
      "Fetched 25000 comments from r/nyc\n",
      "Fetched 26000 comments from r/nyc\n",
      "Fetched 27000 comments from r/nyc\n",
      "Fetched 28000 comments from r/nyc\n",
      "Fetched 29000 comments from r/nyc\n",
      "Fetched 30000 comments from r/nyc\n",
      "Fetched 31000 comments from r/nyc\n",
      "Fetched 32000 comments from r/nyc\n",
      "Fetched 33000 comments from r/nyc\n",
      "Fetched 34000 comments from r/nyc\n",
      "Fetched 35000 comments from r/nyc\n",
      "Fetched 36000 comments from r/nyc\n",
      "Fetched 37000 comments from r/nyc\n",
      "Fetched 38000 comments from r/nyc\n",
      "Fetched 39000 comments from r/nyc\n",
      "Fetched 40000 comments from r/nyc\n",
      "Fetched 41000 comments from r/nyc\n",
      "Fetched 42000 comments from r/nyc\n",
      "Fetched 43000 comments from r/nyc\n",
      "Fetched 44000 comments from r/nyc\n",
      "Fetched 45000 comments from r/nyc\n",
      "Fetched 46000 comments from r/nyc\n",
      "Fetched 47000 comments from r/nyc\n",
      "Fetched 48000 comments from r/nyc\n",
      "Fetched 49000 comments from r/nyc\n",
      "Fetched 50000 comments from r/nyc\n",
      "Fetched 51000 comments from r/nyc\n",
      "Fetched 52000 comments from r/nyc\n",
      "Fetched 53000 comments from r/nyc\n",
      "Fetched 54000 comments from r/nyc\n",
      "Fetched 55000 comments from r/nyc\n",
      "Fetched 56000 comments from r/nyc\n",
      "Fetched 57000 comments from r/nyc\n",
      "Fetched 58000 comments from r/nyc\n",
      "Fetched 59000 comments from r/nyc\n",
      "Fetched 60000 comments from r/nyc\n",
      "Fetched 61000 comments from r/nyc\n",
      "Fetched 62000 comments from r/nyc\n",
      "Fetched 63000 comments from r/nyc\n",
      "Fetched 64000 comments from r/nyc\n",
      "Fetched 65000 comments from r/nyc\n",
      "Fetched 66000 comments from r/nyc\n",
      "Fetched 67000 comments from r/nyc\n",
      "Fetched 68000 comments from r/nyc\n",
      "Fetched 69000 comments from r/nyc\n",
      "Fetched 70000 comments from r/nyc\n",
      "Fetched 71000 comments from r/nyc\n",
      "Fetched 72000 comments from r/nyc\n",
      "Fetched 73000 comments from r/nyc\n",
      "Fetched 74000 comments from r/nyc\n",
      "Fetched 75000 comments from r/nyc\n",
      "Fetched 76000 comments from r/nyc\n",
      "Fetched 77000 comments from r/nyc\n",
      "Fetched 78000 comments from r/nyc\n",
      "Fetched 79000 comments from r/nyc\n",
      "Fetched 80000 comments from r/nyc\n",
      "Fetched 81000 comments from r/nyc\n",
      "Fetched 82000 comments from r/nyc\n",
      "Fetched 83000 comments from r/nyc\n",
      "Fetched 84000 comments from r/nyc\n",
      "Fetched 85000 comments from r/nyc\n",
      "Fetched 86000 comments from r/nyc\n",
      "Fetched 87000 comments from r/nyc\n",
      "Fetched 88000 comments from r/nyc\n",
      "Fetched 89000 comments from r/nyc\n",
      "Fetched 90000 comments from r/nyc\n",
      "Fetched 91000 comments from r/nyc\n",
      "Fetched 92000 comments from r/nyc\n",
      "Fetched 93000 comments from r/nyc\n",
      "Fetched 94000 comments from r/nyc\n",
      "Fetched 95000 comments from r/nyc\n",
      "Fetched 96000 comments from r/nyc\n",
      "Fetched 97000 comments from r/nyc\n",
      "Fetched 97008 comments from r/nyc\n",
      "Fetched 1000 comments from r/houston\n",
      "Fetched 2000 comments from r/houston\n",
      "Fetched 3000 comments from r/houston\n",
      "Fetched 4000 comments from r/houston\n",
      "Fetched 5000 comments from r/houston\n",
      "Fetched 6000 comments from r/houston\n",
      "Fetched 7000 comments from r/houston\n",
      "Fetched 8000 comments from r/houston\n",
      "Fetched 9000 comments from r/houston\n",
      "Fetched 10000 comments from r/houston\n",
      "Fetched 11000 comments from r/houston\n",
      "Fetched 12000 comments from r/houston\n",
      "Fetched 13000 comments from r/houston\n",
      "Fetched 14000 comments from r/houston\n",
      "Fetched 15000 comments from r/houston\n",
      "Fetched 16000 comments from r/houston\n",
      "Fetched 17000 comments from r/houston\n",
      "Fetched 18000 comments from r/houston\n",
      "Fetched 19000 comments from r/houston\n",
      "Fetched 20000 comments from r/houston\n",
      "Fetched 20614 comments from r/houston\n"
     ]
    }
   ],
   "source": [
    "# get comments\n",
    "comments = pd.DataFrame(columns = comment_fields)\n",
    "df_list = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    start_time = round(time.time())\n",
    "    c = 0\n",
    "    while c < submissions[submissions['subreddit'] == subreddit]['num_comments'].sum():\n",
    "        time.sleep(10) # ~6 requests/minute\n",
    "        res = requests.get(\n",
    "            comment_url,\n",
    "            params={\n",
    "                'subreddit' : subreddit,\n",
    "                'fields': comment_fields,\n",
    "                'link_id' : (['t3_' + n for n in link_ids[subreddit]]),\n",
    "                'size' : 1000,\n",
    "                'before' : start_time,\n",
    "            })\n",
    "        # Make sure we got a 2xx response\n",
    "        res.raise_for_status()\n",
    "        \n",
    "        # Don't parse data unless we got at least one post\n",
    "        if len(res.json()['data']) == 0:\n",
    "            break\n",
    "\n",
    "        df = pd.DataFrame(res.json()['data'])\n",
    "\n",
    "        # raise counter by number of rows in df\n",
    "        c += df.shape[0]\n",
    "        \n",
    "        # Add these new comments to our big data frame\n",
    "        df_list.append(df)\n",
    "\n",
    "        # Reset start time so we're getting earlier comments next iteration\n",
    "        start_time = df['created_utc'].min()\n",
    "        \n",
    "        # Show status message\n",
    "        print(f\"Fetched {c} comments from r/{subreddit}\")\n",
    "\n",
    "# Put all comments in one data frame\n",
    "comments = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Convert date to YYYY-MM-DD format\n",
    "comments['date'] = [dt.date.fromtimestamp(x).isoformat() for x in comments['created_utc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewest posts we got from a subreddit\n",
    "smallest = comments['subreddit'].value_counts().min()\n",
    "\n",
    "# Pare every subreddit down to this number by random sampling\n",
    "comments_sampled = pd.concat([\n",
    "        comments[comments['subreddit'] == subreddit].sample(smallest, random_state=seed)\n",
    "        for subreddit in subreddits\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sampled.to_csv('../data/comments_sampled.csv', index=False)\n",
    "comments.to_csv('../data/comments_all.csv', index=False)\n",
    "submissions.to_csv('../data/submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
