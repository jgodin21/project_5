{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit get-comment tool, covid-19 sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "comment_url = 'https://api.pushshift.io/reddit/search/comment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define subreddits, fields gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with different localities here\n",
    "subreddits = ['nyc', 'houston']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_fields = ['id','title', 'created_utc','num_comments','subreddit']\n",
    "comment_fields = ['link_id','body','created_utc', 'subreddit', 'score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set key terms; Name data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch name (suffix to add to all saved data)\n",
    "prefix = '2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search terms\n",
    "keywords = 'covid|quarantine|pandemic|coronavirus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time flags (search starts at t2 and goes back by 'span_days' find t1)\n",
    "# 86400 utc = 1 day\n",
    "\n",
    "# Start Time\n",
    "# t2 = round(time.time()) # Now\n",
    "t2 = round((dt.datetime.now() - dt.timedelta(days=365)).timestamp()) #5/10/2019, 12 am\n",
    "\n",
    "# Search Span\n",
    "span_days = 80\n",
    "\n",
    "t1 = str(int(t2) - span_days*86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get submissions\n",
    "submissions = pd.DataFrame(columns = submission_fields)\n",
    "df_list = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    start_time = previous_date\n",
    "    # start_time = round(time.time())\n",
    "    res = requests.get(\n",
    "        sub_url,\n",
    "        params={\n",
    "            'subreddit' : subreddit,\n",
    "            # 'q' : keywords,\n",
    "            'fields': submission_fields,\n",
    "            'size' : 400,\n",
    "            'sort_type' : 'num_comments',\n",
    "            'sort' : 'desc',\n",
    "            'before': start_time,  \n",
    "            'after': t1,\n",
    "        })\n",
    "    # Make sure we got a 2xx response\n",
    "    res.raise_for_status()\n",
    "\n",
    "    df = pd.DataFrame(res.json()['data'])\n",
    "    \n",
    "    # Filter out non-commented; could also set 'sort_type' parameter to get most commented\n",
    "    df = df[df['num_comments'] >0]\n",
    "    \n",
    "    df_list.append(df)\n",
    "\n",
    "start_time = df.created_utc.min()\n",
    "submissions = pd.concat(df_list, axis=0)\n",
    "submissions['date'] = [dt.date.fromtimestamp(x).isoformat() for x in submissions['created_utc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate id dictionary for cross-referencing submissions with comments\n",
    "link_ids = {sub: submissions[submissions[\"subreddit\"] == sub][\"id\"] for sub in subreddits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 comments from r/nyc since 2019-05-14T18:38:24\n",
      "Fetched 2000 comments from r/nyc since 2019-05-13T14:13:35\n",
      "Fetched 3000 comments from r/nyc since 2019-05-12T11:52:44\n",
      "Fetched 4000 comments from r/nyc since 2019-05-10T13:52:11\n",
      "Fetched 5000 comments from r/nyc since 2019-05-08T17:20:52\n",
      "Fetched 6000 comments from r/nyc since 2019-05-07T20:30:22\n",
      "Fetched 7000 comments from r/nyc since 2019-05-06T15:47:36\n",
      "Fetched 8000 comments from r/nyc since 2019-05-04T10:51:02\n",
      "Fetched 9000 comments from r/nyc since 2019-05-02T00:35:05\n",
      "Fetched 10000 comments from r/nyc since 2019-04-29T21:21:24\n",
      "Fetched 11000 comments from r/nyc since 2019-04-28T00:55:55\n",
      "Fetched 12000 comments from r/nyc since 2019-04-26T09:27:37\n",
      "Fetched 13000 comments from r/nyc since 2019-04-24T20:00:21\n",
      "Fetched 14000 comments from r/nyc since 2019-04-23T13:49:55\n",
      "Fetched 15000 comments from r/nyc since 2019-04-21T21:25:06\n",
      "Fetched 16000 comments from r/nyc since 2019-04-19T12:22:18\n",
      "Fetched 17000 comments from r/nyc since 2019-04-17T15:50:48\n",
      "Fetched 18000 comments from r/nyc since 2019-04-16T00:35:02\n",
      "Fetched 19000 comments from r/nyc since 2019-04-15T10:00:54\n",
      "Fetched 20000 comments from r/nyc since 2019-04-13T18:27:52\n",
      "Fetched 21000 comments from r/nyc since 2019-04-12T11:25:32\n",
      "Fetched 22000 comments from r/nyc since 2019-04-10T18:33:18\n",
      "Fetched 23000 comments from r/nyc since 2019-04-09T21:48:24\n",
      "Fetched 24000 comments from r/nyc since 2019-04-08T11:15:06\n",
      "Fetched 25000 comments from r/nyc since 2019-04-07T17:00:34\n",
      "Fetched 26000 comments from r/nyc since 2019-04-05T18:47:44\n",
      "Fetched 27000 comments from r/nyc since 2019-04-04T12:55:53\n",
      "Fetched 28000 comments from r/nyc since 2019-04-03T10:30:12\n",
      "Fetched 29000 comments from r/nyc since 2019-04-01T23:52:47\n",
      "Fetched 30000 comments from r/nyc since 2019-03-31T21:21:03\n",
      "Fetched 31000 comments from r/nyc since 2019-03-29T21:30:39\n",
      "Fetched 32000 comments from r/nyc since 2019-03-28T17:10:39\n",
      "Fetched 33000 comments from r/nyc since 2019-03-27T21:49:36\n",
      "Fetched 34000 comments from r/nyc since 2019-03-27T00:27:32\n",
      "Fetched 35000 comments from r/nyc since 2019-03-25T18:58:57\n",
      "Fetched 36000 comments from r/nyc since 2019-03-24T10:18:05\n",
      "Fetched 37000 comments from r/nyc since 2019-03-22T13:26:49\n",
      "Fetched 38000 comments from r/nyc since 2019-03-21T09:10:05\n",
      "Fetched 39000 comments from r/nyc since 2019-03-19T14:11:54\n",
      "Fetched 40000 comments from r/nyc since 2019-03-18T17:47:04\n",
      "Fetched 41000 comments from r/nyc since 2019-03-16T22:25:36\n",
      "Fetched 42000 comments from r/nyc since 2019-03-14T23:48:20\n",
      "Fetched 43000 comments from r/nyc since 2019-03-14T06:23:30\n",
      "Fetched 44000 comments from r/nyc since 2019-03-12T15:41:39\n",
      "Fetched 45000 comments from r/nyc since 2019-03-11T21:34:29\n",
      "Fetched 46000 comments from r/nyc since 2019-03-11T09:23:29\n",
      "Fetched 47000 comments from r/nyc since 2019-03-09T10:51:27\n",
      "Fetched 48000 comments from r/nyc since 2019-03-08T13:32:19\n",
      "Fetched 49000 comments from r/nyc since 2019-03-07T16:31:08\n",
      "Fetched 50000 comments from r/nyc since 2019-03-07T00:15:20\n",
      "Fetched 51000 comments from r/nyc since 2019-03-06T12:59:13\n",
      "Fetched 52000 comments from r/nyc since 2019-03-05T16:19:12\n",
      "Fetched 53000 comments from r/nyc since 2019-03-04T18:17:00\n",
      "Fetched 54000 comments from r/nyc since 2019-03-03T16:05:54\n",
      "Fetched 55000 comments from r/nyc since 2019-03-01T20:12:48\n",
      "Fetched 56000 comments from r/nyc since 2019-02-28T19:17:00\n",
      "Fetched 57000 comments from r/nyc since 2019-02-27T07:42:33\n",
      "Fetched 58000 comments from r/nyc since 2019-02-25T13:28:50\n",
      "Fetched 58055 comments from r/nyc since 2019-02-23T23:46:53\n",
      "Fetched 1000 comments from r/houston since 2019-05-14T18:38:24\n",
      "Fetched 2000 comments from r/houston since 2019-05-12T15:18:14\n",
      "Fetched 3000 comments from r/houston since 2019-05-10T12:19:41\n",
      "Fetched 4000 comments from r/houston since 2019-05-09T23:02:43\n",
      "Fetched 5000 comments from r/houston since 2019-05-08T22:20:58\n",
      "Fetched 6000 comments from r/houston since 2019-05-07T21:49:20\n",
      "Fetched 7000 comments from r/houston since 2019-05-06T14:41:11\n",
      "Fetched 8000 comments from r/houston since 2019-05-03T11:11:56\n",
      "Fetched 9000 comments from r/houston since 2019-05-01T23:13:24\n",
      "Fetched 10000 comments from r/houston since 2019-04-30T16:16:33\n",
      "Fetched 11000 comments from r/houston since 2019-04-27T10:06:06\n",
      "Fetched 12000 comments from r/houston since 2019-04-25T23:58:13\n",
      "Fetched 13000 comments from r/houston since 2019-04-24T17:24:35\n",
      "Fetched 14000 comments from r/houston since 2019-04-23T09:37:51\n",
      "Fetched 15000 comments from r/houston since 2019-04-19T23:04:21\n",
      "Fetched 16000 comments from r/houston since 2019-04-18T19:55:07\n",
      "Fetched 17000 comments from r/houston since 2019-04-17T20:47:58\n",
      "Fetched 18000 comments from r/houston since 2019-04-15T19:06:59\n",
      "Fetched 19000 comments from r/houston since 2019-04-12T23:35:05\n",
      "Fetched 20000 comments from r/houston since 2019-04-11T15:00:58\n",
      "Fetched 21000 comments from r/houston since 2019-04-10T08:53:46\n",
      "Fetched 22000 comments from r/houston since 2019-04-07T21:23:58\n",
      "Fetched 23000 comments from r/houston since 2019-04-04T15:21:56\n",
      "Fetched 24000 comments from r/houston since 2019-04-02T14:08:46\n",
      "Fetched 25000 comments from r/houston since 2019-03-31T13:58:38\n",
      "Fetched 26000 comments from r/houston since 2019-03-29T12:02:45\n",
      "Fetched 27000 comments from r/houston since 2019-03-27T12:34:47\n",
      "Fetched 28000 comments from r/houston since 2019-03-25T10:04:11\n",
      "Fetched 29000 comments from r/houston since 2019-03-22T22:13:28\n",
      "Fetched 30000 comments from r/houston since 2019-03-22T12:51:11\n",
      "Fetched 31000 comments from r/houston since 2019-03-21T13:15:52\n",
      "Fetched 32000 comments from r/houston since 2019-03-20T19:10:09\n",
      "Fetched 33000 comments from r/houston since 2019-03-19T22:48:44\n",
      "Fetched 34000 comments from r/houston since 2019-03-19T17:38:54\n",
      "Fetched 35000 comments from r/houston since 2019-03-19T13:33:03\n",
      "Fetched 36000 comments from r/houston since 2019-03-19T00:26:10\n",
      "Fetched 37000 comments from r/houston since 2019-03-18T12:28:14\n",
      "Fetched 38000 comments from r/houston since 2019-03-15T13:02:38\n",
      "Fetched 39000 comments from r/houston since 2019-03-12T11:59:06\n",
      "Fetched 40000 comments from r/houston since 2019-03-10T13:19:23\n",
      "Fetched 41000 comments from r/houston since 2019-03-08T10:48:07\n",
      "Fetched 42000 comments from r/houston since 2019-03-06T18:58:10\n",
      "Fetched 43000 comments from r/houston since 2019-03-04T15:27:55\n",
      "Fetched 44000 comments from r/houston since 2019-03-02T01:15:09\n",
      "Fetched 45000 comments from r/houston since 2019-02-27T14:39:36\n",
      "Fetched 45381 comments from r/houston since 2019-02-25T18:50:52\n"
     ]
    }
   ],
   "source": [
    "# get comments\n",
    "df_list = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    start_time = previous_date\n",
    "    c = 0\n",
    "    while c < submissions[submissions['subreddit'] == subreddit]['num_comments'].sum():\n",
    "        time.sleep(2)\n",
    "        res = requests.get(\n",
    "            comment_url,\n",
    "            params={\n",
    "                'subreddit' : subreddit,\n",
    "                'fields': comment_fields,\n",
    "                'link_id' : (['t3_' + n for n in link_ids[subreddit]]),  #/comment?link_id : /submission?ids\n",
    "                'size' : 1000,\n",
    "                'before' : start_time,\n",
    "            })\n",
    "        # Make sure we got a 2xx response\n",
    "        res.raise_for_status()\n",
    "\n",
    "        # Don't parse data unless we got at least one post\n",
    "        if len(res.json()['data']) == 0:\n",
    "            break\n",
    "        \n",
    "        df = pd.DataFrame(res.json()['data'])\n",
    "\n",
    "        # raise counter by number of rows in df\n",
    "        c += df.shape[0]\n",
    "\n",
    "        print(f\"Fetched {c} comments from r/{subreddit} since {dt.datetime.fromtimestamp(start_time).isoformat()}\")\n",
    "\n",
    "        df_list.append(df)\n",
    "        start_time = df['created_utc'].min()\n",
    "        \n",
    "comments = pd.concat(df_list, axis=0)\n",
    "comments['date'] = [dt.date.fromtimestamp(x).isoformat() for x in comments['created_utc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample/Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewest posts we got from a subreddit\n",
    "smallest = comments['subreddit'].value_counts().min()\n",
    "\n",
    "# Pare every subreddit down to this number by random sampling\n",
    "comments_sampled = pd.concat([\n",
    "        comments[comments['subreddit'] == subreddit].sample(smallest, random_state=101)\n",
    "        for subreddit in subreddits\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "houston    45381\n",
       "nyc        45381\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify we now have equal classes\n",
    "comments_sampled['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local hard drive with prefix\n",
    "today = dt.date.today().isoformat()\n",
    "\n",
    "comments.to_csv(f'../data/{prefix}_reddit-comments_all-{today}.csv.bz2', index=False, compression='bz2')\n",
    "comments_sampled.to_csv(f'../data/{prefix}_reddit-comments_sampled-{today}.csv.bz2', index=False, compression='bz2')\n",
    "submissions.to_csv(f'../data/{prefix}_reddit-submissions-{today}.csv.bz2', index=False, compression='bz2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
